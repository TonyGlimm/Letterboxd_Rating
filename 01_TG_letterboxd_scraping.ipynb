{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_TG_letterboxd_scraping.ipynb",
      "provenance": [],
      "mount_file_id": "1K6eOzWMVKb-24Do4szbG6CdiaY6kZqk4",
      "authorship_tag": "ABX9TyMHI2uyEHYJIprHZrmGgbAq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('drive', force_remount=True)\n",
        "from pathlib import Path\n",
        "import lxml\n",
        "_domain = 'https://letterboxd.com/'"
      ],
      "metadata": {
        "id": "hHlS1ciuZte-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93480a68-af0b-48a6-eff2-9a4ef6fb8f90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_stars(starstring):\n",
        "    \"\"\"\n",
        "    Transforms star rating into float value\n",
        "    \"\"\"\n",
        "    stars = {\n",
        "        \"★\": 1,\n",
        "        \"★★\": 2,\n",
        "        \"★★★\": 3,\n",
        "        \"★★★★\": 4,\n",
        "        \"★★★★★\": 5,\n",
        "        \"½\": 0.5,\n",
        "        \"★½\": 1.5,\n",
        "        \"★★½\": 2.5,\n",
        "        \"★★★½\": 3.5,\n",
        "        \"★★★★½\": 4.5\n",
        "    }\n",
        "    try:\n",
        "        return stars[starstring]\n",
        "    except:\n",
        "        return np.nan"
      ],
      "metadata": {
        "id": "nZufmUNV45oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_user_list(num_users_page):\n",
        "  url = 'https://letterboxd.com/members/popular/this/all-time/'\n",
        "  users = []\n",
        "  for i in range(num_users_page):\n",
        "\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "      print('Scraping page '+str(i+1)+' of '+str(num_users_page))\n",
        "    elif response.satus_code ==404:\n",
        "      print(\"not found.\")\n",
        "\n",
        "\n",
        "    #finding user name\n",
        "    user_page = url  \n",
        "    userget = requests.get(user_page)\n",
        "    user_soup = BeautifulSoup(userget.content, 'lxml')\n",
        "\n",
        "    body = user_soup.find(\"body\")\n",
        "\n",
        "    for a in user_soup.find_all('a', class_='name', href=True):\n",
        "      user_temp = a['href']\n",
        "      user = user_temp.replace(\"/\",\"\")\n",
        "      \n",
        "\n",
        "      users.append(user)\n",
        "      users=list(set(users))   # for some reason it pulls the first 5 entries twice. dont know why. this deletes duplicates. it destroys my order but that doesnt matter\n",
        "\n",
        "      # check if there is another page of users\n",
        "    next_button = user_soup.find('a', class_='next')\n",
        "    if next_button is None:\n",
        "      #print('none')\n",
        "      condition = False\n",
        "    else:\n",
        "      url = _domain + next_button['href']\n",
        "      user_soup.decompose\n",
        "\n",
        "  return users"
      ],
      "metadata": {
        "id": "bDWiGEeMiKKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_num(user):\n",
        "\n",
        "\n",
        "  link = 'https://letterboxd.com/'+user+'/films/ratings/'\n",
        "  \n",
        "  response = requests.get(link)\n",
        "  if response.status_code == 200:\n",
        "    #print('Response Success!')\n",
        "    pass\n",
        "  elif response.status_code ==404:\n",
        "    print(\"not found.\")\n",
        "\n",
        "  soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "  body = soup.find(\"body\")\n",
        "\n",
        "  try:\n",
        "        page_link = soup.findAll(\"li\", attrs={\"class\", \"paginate-page\"})[-1]\n",
        "        num_page = int(page_link.find(\"a\").text.replace(',', ''))\n",
        "        display_name = body.find(\"section\", attrs={\"id\": \"profile-header\"}).find(\"h1\", attrs={\"class\": \"title-3\"}).text.strip()\n",
        "  except IndexError:\n",
        "        num_page = 1\n",
        "        display_name = None\n",
        "\n",
        "  print('Number of pages for user ' + \"'\" +user +\"'\" + ' is ' +str(num_page))\n",
        "\n",
        "  return num_page"
      ],
      "metadata": {
        "id": "8JBp1deriMMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(username):\n",
        "  '''\n",
        "  main function\n",
        "  needs username and returns movie_DF and user_DF (Pandas DF) with information from 1 user which has all of the informations\n",
        "  '''\n",
        "  film_names =[]\n",
        "  ratings =[]\n",
        "  release_years =[]\n",
        "  directors =[]\n",
        "  casts=[]\n",
        "  genres = []\n",
        "  themes = []\n",
        "  average_ratings=[]\n",
        "  URLs =[]\n",
        "  lb_users = []\n",
        "\n",
        "  num_pages = get_page_num(username)\n",
        "\n",
        "  for i in range(num_pages):\n",
        "    link = 'https://letterboxd.com/'+username+'/films/ratings/page/'+str(i+1)\n",
        "    print(link)\n",
        "    \n",
        "\n",
        "    response = requests.get(link)\n",
        "    if response.status_code == 200:\n",
        "      print('Scraping page '+str(i+1)+' of '+str(num_pages))\n",
        "    elif response.status_code ==404:\n",
        "      print(\"not found.\")\n",
        "    soup = BeautifulSoup(response.content, 'lxml')\n",
        "    table = soup.find('ul', class_='poster-list')\n",
        "    films = table.find_all('li')\n",
        "\n",
        "    for film in tqdm(films):\n",
        "      #finding film name\n",
        "      panel = film.find('div').find('img')\n",
        "      film_name = panel['alt']\n",
        "\n",
        "      #try to find the rating of a film if possible and converting to float\n",
        "      try:\n",
        "        stars = film.find('span', class_='rating').get_text().strip()\n",
        "        rating = transform_stars(stars)\n",
        "      except:\n",
        "        rating =np.nan\n",
        "      \n",
        "      # find each film \"card\" from the grid of the user list go through each film and pull the data from the individual film page\n",
        "      film_card = film.find('div').get('data-target-link')\n",
        "      film_page = _domain + film_card #theoratically we have a '/' too much in there but it works. no reason for cleanup right now\n",
        "      filmget = requests.get(film_page)\n",
        "      film_soup = BeautifulSoup(filmget.content, 'lxml')\n",
        "\n",
        "      release_year = film_soup.find('meta', attrs={'property':'og:title'}).attrs['content'][-5:-1]\n",
        "      director = film_soup.find('meta', attrs={'name':'twitter:data1'}).attrs['content']\n",
        "      \n",
        "      #find cast\n",
        "      try:\n",
        "        cast = [ line.contents[0] for line in film_soup.find('div', attrs={'id':'tab-cast'}).find_all('a')]\n",
        "\n",
        "        #remove all the 'Show all' tags if they are present\n",
        "        cast =[i for i in cast if i != 'Show All...']\n",
        "        cast = cast[0:5]                                                        #started with 8 and reduced to 5 now\n",
        "      except:\n",
        "        cast =np.nan     \n",
        "\n",
        "      try:\n",
        "        average_rating = float(film_soup.find('meta', attrs={'name':'twitter:data2'}).attrs['content'][:4])\n",
        "      except:\n",
        "        average_rating = np.nan\n",
        "\n",
        "      try:  \n",
        "        genre = [ line.contents[0] for line in film_soup.find('div', attrs={'id':'tab-genres'}).find_all('a')]\n",
        "        genre =[i for i in genre if i != 'Show All...']\n",
        "        #turns out that we also have \"themes\" which are under genre. putting the themes in as genres bloats up my features,\n",
        "        #so I will split it here into genres and themes. on average every movie has 3 themes so that is where I split. solving this via the scraping is a To-Do \n",
        "        if len(genre)>3:\n",
        "          theme= genre[3:]\n",
        "        else:\n",
        "          theme=np.nan\n",
        "        genre = genre[:3]\n",
        "      except:\n",
        "        genre = np.nan  \n",
        "      \n",
        "      URL= _domain+film_card\n",
        "\n",
        "      film_soup.decompose\n",
        "\n",
        "      film_names.append(film_name)\n",
        "      release_years.append(release_year)\n",
        "      directors.append(director)\n",
        "      casts.append(cast)\n",
        "      average_ratings.append(average_rating)\n",
        "      URLs.append(_domain+film_card)\n",
        "      lb_users.append(username)\n",
        "      ratings.append(rating)\n",
        "      genres.append(genre)\n",
        "      themes.append(theme)\n",
        "\n",
        "    movie_DF =pd.DataFrame({'Title': film_names, 'Average_Rating': average_ratings, 'Release_Year': release_years, 'Director': directors,'Cast':casts, 'Genres': genres, 'Themes':themes})\n",
        "    user_DF = pd.DataFrame({'Title': film_names,'User_Rating': ratings, 'lb_username':lb_users})\n",
        "\n",
        "  return movie_DF, user_DF"
      ],
      "metadata": {
        "id": "IU9lHc9GiOGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_movie_db = 'drive/My Drive/Colab Notebooks/data/movie_db.csv' \n",
        "path_to_user_db =  'drive/My Drive/Colab Notebooks/data/user_ratings.csv'\n",
        "path1 = Path(path_to_movie_db)\n",
        "path2 = Path(path_to_user_db)"
      ],
      "metadata": {
        "id": "VgUnxUQ5iPn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users = get_user_list(1)\n",
        "#users.append('swaiborr','behaind', 'csb_de', 'davidehrlich','awesometacular','ignmovies','chrisstuckmann9','robert_hofmann_') specifically pulling these for my test_set when building the ML-model\n",
        "#users=list(set(users)) # to get rid of duplicates if I create some with the line above"
      ],
      "metadata": {
        "id": "4PKfiYt7iQV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Do \n",
        "create a check if user already exists in csv file. if user already exists delete them from 'users' so we dont gather data from a user we already have data on"
      ],
      "metadata": {
        "id": "fqfHD3F1iZ01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if path2.is_file():\n",
        "  old_user_df = pd.read_csv(path_to_user_db)\n",
        "  old_users = list(old_user_df['lb_username']) \n",
        "else:\n",
        "  pass\n",
        "\n",
        "for old in old_users:\n",
        "  if old in users: users.remove(old)\n",
        " #this should delete every user in users which is already in my \"Database\". this way I wont scrape the data from them twice. this is a quite brute way of solving this because it doesnt check if the user has new ratings since I last pulled the data"
      ],
      "metadata": {
        "id": "3UPPg8JtiqxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users"
      ],
      "metadata": {
        "id": "XUrc_8RCiS3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_df= pd.DataFrame()\n",
        "user_df= pd.DataFrame()\n",
        "\n",
        "for user in users:\n",
        "  print(user)\n",
        "  temp_movie_df, temp_user_df = get_data(user)\n",
        "  \n",
        "  if path1.is_file():\n",
        "    temp_movie_df.to_csv(\"drive/My Drive/Colab Notebooks/data/movie_db.csv\", mode='a', index=False, header=False) \n",
        "  else:\n",
        "    temp_movie_df.to_csv(\"drive/My Drive/Colab Notebooks/data/movie_db.csv\",index=False, header=True)\n",
        "    \n",
        "  if path2.is_file():\n",
        "    temp_user_df.to_csv(\"drive/My Drive/Colab Notebooks/data/user_ratings.csv\", mode='a', index=False, header=False) \n",
        "  else:\n",
        "    temp_user_df.to_csv(\"drive/My Drive/Colab Notebooks/data/user_ratings.csv\",index=False, header=True)\n",
        "\n",
        "  #movie_df= pd.concat([movie_df, temp_movie_df],ignore_index=True)\n",
        "  #user_df= pd.concat([user_df, temp_user_df],ignore_index=True)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "B-ToQ33TiXHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}